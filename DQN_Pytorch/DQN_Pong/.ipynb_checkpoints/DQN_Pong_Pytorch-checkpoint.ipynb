{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import cv2\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: RepeatActionAndMaxFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
    "    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,\n",
    "                 fire_first=False):\n",
    "        \"\"\"\n",
    "        env: Environment to add wrapper around\n",
    "        repeat: Number of times each action is executed \n",
    "        clip_reward: Boolean. If True, clip reward to +/-1 range\n",
    "        no_ops:\n",
    "        fire_first:\n",
    "        \"\"\"\n",
    "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.low.shape # Shape of observation from environment\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape)) # Initialize frame_buffer\n",
    "        self.clip_reward = clip_reward\n",
    "        self.no_ops = no_ops\n",
    "        self.fire_first = fire_first\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes action self.repeat times and captures total reward in t_reward\n",
    "        Reward is clipped if self.clip_reward is True\n",
    "        \"\"\"\n",
    "        t_reward = 0.0 # Initialize total reward over repeated actions to zero\n",
    "        done = False\n",
    "        for i in range(self.repeat): # Repeat actions 'self.repeat' times\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
    "            t_reward += reward # Increment t_reward with current reward\n",
    "            idx = i % 2\n",
    "            self.frame_buffer[idx] = obs # Frame_buffer updated with last two observations\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1]) # Take maximum of last two frames\n",
    "        return max_frame, t_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Wrapper for environment reset\n",
    "        \"\"\"\n",
    "        obs = self.env.reset() # Reset environment\n",
    "        no_ops = np.random.randint(self.no_ops) + 1 if self.no_ops > 0 else 0\n",
    "        for _ in range(no_ops):\n",
    "            _, _, done, _ = self.env.step(0)\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "        if self.fire_first:\n",
    "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            obs, _, _, _ = self.env.step(1)\n",
    "\n",
    "        self.frame_buffer = np.zeros_like((2,self.shape)) # Initialize frame buffer to zeros\n",
    "        self.frame_buffer[0] = obs # Update frame buffer with first observation after reset\n",
    "\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: preprocessFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, shape, env=None):\n",
    "        \"\"\"\n",
    "        Class to pre-process frame.\n",
    "        Frame converted from RGB to grayscale, resized to new shape in C,H,W format\n",
    "        \n",
    "        Arguments:\n",
    "            shape: Shape to pre-process frame to\n",
    "            env: Environment which will be wrapped\n",
    "        \"\"\"\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.shape = (shape[2], shape[0], shape[1]) # Change to C,H,W format\n",
    "        # Change observation space to match new shape and dtype to float32 with range = [0.0, 1.0]\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,shape=self.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        obs converted from RGB to grayscale and resized to new shape in C,H,W format\n",
    "        \n",
    "        Argument:\n",
    "            obs: Input Frame in color format\n",
    "        Output:\n",
    "            new_obs: Output Frame in grayscale and C,H,W format\n",
    "        \"\"\"\n",
    "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY) # Convert frame to grayscale\n",
    "        resized_screen = cv2.resize(new_frame, self.shape[1:], interpolation=cv2.INTER_AREA) # Resize frame to new shape\n",
    "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
    "        new_obs = new_obs / 255.0 # Convert to float32 in range [0.0, 1.0]\n",
    "\n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: StackFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackFrames(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Class to return stack of last 4 observations\n",
    "    \"\"\"\n",
    "    def __init__(self, env, repeat):\n",
    "        \"\"\"\n",
    "        Arguments: \n",
    "            env: Environment to wrap\n",
    "            repeat: Number of observations to stack\n",
    "        \"\"\"\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                            env.observation_space.low.repeat(repeat, axis=0),\n",
    "                            env.observation_space.high.repeat(repeat, axis=0),\n",
    "                            dtype=np.float32)\n",
    "        self.stack = collections.deque(maxlen=repeat)\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Append latest observation to stack\n",
    "        \"\"\"\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: DeepQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "\n",
    "        fc_input_dims = self.calculate_conv_output_dims(input_dims)\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_input_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def calculate_conv_output_dims(self, input_dims):\n",
    "        state = T.zeros(1, *input_dims)\n",
    "        dims = self.conv1(state)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self, state):\n",
    "        conv1 = F.relu(self.conv1(state))\n",
    "        conv2 = F.relu(self.conv2(conv1))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        # conv3 shape is BS x n_filters x H x W\n",
    "        conv_state = conv3.view(conv3.size()[0], -1)\n",
    "        # conv_state shape is BS x (n_filters * H * W)\n",
    "        flat1 = F.relu(self.fc1(conv_state))\n",
    "        actions = self.fc2(flat1)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
    "                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):\n",
    "        self.gamma = gamma # Discount Factor\n",
    "        self.epsilon = epsilon # for eps-greedy action selection\n",
    "        self.lr = lr # Learning Rate\n",
    "        self.n_actions = n_actions # Number of actions\n",
    "        self.input_dims = input_dims # Dimension of observations from wrapped environment\n",
    "        self.batch_size = batch_size # Batch size to use while training\n",
    "        self.eps_min = eps_min # Minimum eps-value to use while training\n",
    "        self.eps_dec = eps_dec # Decay rate of epsilon\n",
    "        self.replace_target_cnt = replace # Rate at which target network will be replaced\n",
    "        self.algo = algo # Name of learning algorithm\n",
    "        self.env_name = env_name # Name of environment\n",
    "        self.chkpt_dir = chkpt_dir # Directory to save checkpoint files\n",
    "        self.action_space = [i for i in range(n_actions)] # Action space\n",
    "        self.learn_step_counter = 0 # Number of learning steps completed so far\n",
    "\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims) # Instantiate replay buffer object\n",
    "\n",
    "        # Instantiate main DQN\n",
    "        self.q_eval = DeepQNetwork(self.lr, self.n_actions, input_dims=self.input_dims,\\\n",
    "                                   name=self.env_name+'_'+self.algo+'_q_eval', chkpt_dir=self.chkpt_dir)\n",
    "        # Instantiate target DQN\n",
    "        self.q_next = DeepQNetwork(self.lr, self.n_actions, input_dims=self.input_dims,\\\n",
    "                                   name=self.env_name+'_'+self.algo+'_q_next', chkpt_dir=self.chkpt_dir)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        Perform epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        if np.random.random() > self.epsilon: # Choose greedy action with probability '1 - epsilon'\n",
    "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
    "            actions = self.q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else: # Choose an action randomly with probability 'epsilon'\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        \"\"\"\n",
    "        Update replay memory buffer with latest entry\n",
    "        \"\"\"\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        \"\"\"\n",
    "        Update target DQN weights with main DQN weights every 'replace_target_cnt' steps\n",
    "        \"\"\"\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "            \n",
    "    def sample_memory(self):\n",
    "        \"\"\"\n",
    "        Sample a random batch from replay memory\n",
    "        \"\"\"\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "        return states, actions, rewards, states_, dones            \n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decrease epsilon after every learning step and clamp at eps_min\n",
    "        \"\"\"\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_models(self):\n",
    "        \"\"\"\n",
    "        Save checkpoints for both DQN models\n",
    "        \"\"\"\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"\n",
    "        Load checkpoints for both DQN models\n",
    "        \"\"\"\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Perform one learning step\n",
    "        \"\"\"\n",
    "        # Start learning only after replay_memory has atleast 'batch_size' elements\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.q_eval.optimizer.zero_grad() # Reset optimizer gradients\n",
    "        self.replace_target_network() # Replace target DQN weights with main DQN weights\n",
    "        states, actions, rewards, states_, dones = self.sample_memory() # Sample a new batch from replay memory\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions] # Q-values of current states\n",
    "        q_next = self.q_next.forward(states_).max(dim=1)[0] # Q-values of next states\n",
    "        q_next[dones] = 0.0 # Set terminal state q-values to 0.\n",
    "        q_target = rewards + self.gamma*q_next # # Target Q-values for current states\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device) # MSE Loss between current and target Q-values\n",
    "        loss.backward() # Compute gradients\n",
    "        self.q_eval.optimizer.step() # Update weights\n",
    "        \n",
    "        self.learn_step_counter += 1 # Increment learning step counter by 1.\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False, no_ops=0, fire_first=False):\n",
    "#     env = gym.make(env_name, render_mode = 'human') # Instantiate original environment from gym\n",
    "    env = gym.make(env_name) # Instantiate original environment from gym    \n",
    "    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first) # Wrapper to repeat action, clip_rewards \n",
    "                                                                                 # and return max of latest two frames\n",
    "    env = PreprocessFrame(shape, env) # Convert observation to grayscale and scale to new dimension in C,H,W format\n",
    "    env = StackFrames(env, repeat) # Stack last n observations\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Function: make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 84, 84) 0.0 False {'lives': 0, 'episode_frame_number': 8, 'frame_number': 8}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAAEUCAYAAABd4vGCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmO0lEQVR4nO3dbYxs930f9u9vZnbvEx9ESiRFiXJoG4RkRYWk9MKRo6B1TStwUsfUG6Vy4eIiUME3aWOnKVI6fRHkRQG9CFylaBGAsB0ziKNEUSSQUBPHBB2hLerKpi0nsUzLtCVLokjx0pRMUpf33t2d+ffFzPI+3519mJ2zZz4fYHBmzs7O+f3vmfvdOb85D9VaCwAAAAD9M1h2AQAAAAAshsYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE/tq/FTVT9WVV+uqj+sqkcOqiiA3ZBFQBfIIqAr5BFwuWqt7e0Xq4ZJ/iDJh5I8l+Q3k/xka+33Dq48gJuTRUAXyCKgK+QRcLXRPn73B5P8YWvtK0lSVf88yUNJbhgo63WsHc+pnV+5kjp+POPjw7RBprfaR6WLULNbkskoWTuxmWPDrT2/3FYb5sLFtdRmJS2pSZLJgVTKftSV99t+D45sSbXp9PJ5R82Fs8/9SWvtrmXXMSOLZFH/yaLrWpksSlJro2Q0mmZQ1ZXviS6omr6FKmnDymS0z/dpSwZbyWDcpu/NltQev6zk4LSavfEqmaxN1/N+1Hi6nqu1y3Lp6K3n1777fJeyKNllHsmim5BFnSSLru9mWbSff6K3J/nGZY+fS/Lnb/YLx3Mqf74e3PGFazRKvfOBvPKu2zM+Vtk6kWwd71awtEEyWZtOL9w1yVvffTbff/uf7Pn1vn3xVL701bdl/fn1DDYrw/PJ8OIBFsze1HRDvw2Syfr0tp8N/8FWMtiYbkxv347ixtbv/q//w9eWXcNlZJEs6j9ZdF2rkkWpyuiuezK558600SCT9WHaWrdO09iq0obT28atw5x76zBbJ/f+eoPN5MSfTHL8O+PpB/LNSQZbutDLNhkOZn93Bjl/1ygX3lz72qhee63l5EuTDC9OMthqGWxMjuRG9a99/n/uUhYlu8wjWXRjsqibZNH13SyL9tP4ud5Hzmv+darq4SQPJ8nx0W0Zfv/37/zKw0Eu3HUy42OV8VoyGXZrQyvJpW/ZK2nHWt5+yyv5T2795p5f7lvrt+cPTtyVNlzPZNIyrK4NmDZIJqO2r/dim3TwvXz0ySJZtFJkUWftPosGt2T45jfP8cqDtFtOpo0GaaPB9Fv2rtluTs6+ZR8fS7ZO7P3lBqNkvFZpgzqS37r22mCaH+P16Trez8bWYGP6Tf1gs9JqlmtW90HYMY9k0XxkUYfJol3ZT+PnuSTvuOzxfUmev/pJrbVHkzyaJKfe8o728gfu3vGFW02/zRyvzzp3HcyUZPvDd9JObuU/v/MP8ldveWbPr/WVzdvy/73p/nxr/WSGqe4dTkLacP/fsqe1DDYqNfuDZDUfCFkki1aKLOqsXWfRbafe1ib33zvXi7fRIJPR7FNtFze2ktmH8Mp4Pdm8Ndm4fe/fig83KluvVCZrlUE6eJjtCmqzw3q2N6i3TlQ2bm9pw71vHdV4kMlapW22aUOag7JjHsmi+cii7pFFe7Ofxs9vJnmgqr43yTeTfDTJf32zX5gMkwt39uMfcruTnEoGo0neOnol3zO65Zrnjdu1QTOsa9uRm+3VnFzbTAaXXpdumZ7jZZ/fsg8uBZVVfGBkkSxaKbKos3adRRlUxqfWDqG0wzM9JLEyWWtp69f5EH69z+XXeRNOJtMP9NuHOGbgndoZNf0ypI2SyXpLG121Um+27XX5amzTQ5Unw9neFH37en25dpdHsmhKFh0tsmhX9tz4aa1tVdV/l+TfJhkm+cXW2pcOrLIe+I2Lm/k3r74337nswNK1GueDtz6bv3j8xRyvYY7VWtZquMQq2Zc23SWwrjqXbhtOQ2g/nWfmI4t2JotWgCxaOlm0s+G5QdZerQzGl+a1SrZOtWydarONqv01NVmu2qqsvTrI6PUr50/Wks1bWybH2uxLBZm0SPLo5mRR/8mia+3r/NettX+d5F8fUC29Mm6T/J+vvi//5Nc/mLXvXNqYmqy1/If3vz1v+77P5q3Di7l90HJ77ePAU5aqJpXR65XhhSvnT0bTUJnuiugPx6LJohuTRatBFnWDLLqJlqy/Urn9jyZZe/3SHoiTUeW1+4Y5d980l1oq0ag8sgYblVPPt9zy/PiKL80vvmmQV+8fZGPYpnuUDmulNriWQR7dgCxaCbLoWvu88Bk389LGrVl/eZjjL136pD0+Xnnp3KlcaGvZbBczcZKwo61duvzf5aHSBiuTIRwBsmgFyCKOgMFWsnZukvXXNt+YNxkNMtwYTPuSk0w/iC+vRPapWjK8kKy9tnXFFXEm6+up8bRJnWrWMUsli/pPFl2rW9ffAwAAAODAaPwAAAAA9JTGDwAAAEBPafwAAAAA9JTGDwAAAEBPafwAAAAA9JTGDwAAAEBPafwAAAAA9NRo2QX02SAtbZC04aV5bZAMB215RXHgWs3u1E2fBksji1aDLKLrWiWppA0uvUnb8LI3rPduP2yv48v+xDRfNdMhsmhFyKIraPws0G2j89m6ZZKNjUtbW5NjLbccu5j1jDNIMijJcqRVkkEyGV45uw0u2wiDJZNFK0AWcQS0QbJ1vFLjS2/UNqxM1ipt9snc+/Voa5WM15PxicEVG1vj9ZJHdIYs6j9ZdC2NnwU6OdxIOznO1sald9ZkveXU2kbWapzhCr7h+ujqPSm250FXyKLVIIvoujacfugebF16Y05G01tqNT+I99FkVBkfu2pja62mJ5ioZj2zdLJoNciiK2n8LNAdo3M5+abzeX1w/I15g7VJ7jn+Wo7XOGtJhvYlPPLe2Ni6fDfCYZxBi86QRatBFtF1bTj9lv3yuGmDymRt1qQUQ0deGyST9WTz5JWHV4yPX9qohmWTRf0ni66l8bMgwxrkh0/+QYY/MMkr45NvzF+rcd574mt527DlWK1nrYY3eRW6rg1axseSydpVP6jZIRfOocKSyaLVIIvovEo2b2k5945KbV12Xo1BMj7R0tZa2qCt5IfxPmmjlgtvSTZvGVw1P9k62dKG2+dXkUksiSxaCbLoWho/C/Rn10/kXWvfuGb+sAZJTl77Cxw9lekfiGXXATchi1aALOIImJyY5OLx6/zABlZvtFHL5m0tm7fd4AmXr2uBxZLIov6TRdfS+NmjaklNkkySycYwf3jxnnxp/cU9v94fbd6dVy4cT23NXndF3oBHSU2SGte+/ihsr9sVai6zYLJo9cgiuqxaUuOW4cVBxuf3fpzhcCMZbLbp+32SZOLN2hmtpcaVwWYyvFBXXg1pV68zW8/jlpo0f284ULJoBciiXdH42avtja2tZPDKKJ/9xnvz72+/b88v9+2LJ/PSi7dn/XxlsFXT16ZTtkNlP1tKg83KYJxk+48H7JcsWjmyiE5qSW21ZNCy9vogx19qGZ3be3dysJWsvzbJcGMyfZ/29IP4UVKtJZPpukm1rL/SMhnVvk4iv/5ay+h8y2CjZTDu7wYXh0gW9Z4s2huNn33Y/rA8Ol85+9Jtee38sT2/1ubGKINXRxlsVmrsg3jXbO9VMdhMso/LXtdWrtzQ6mGocPhk0eqQRXTZ9MN4Zbgxyfp3p9/C7tVgnIwutFkO9fND+JHUMtuwnmTt/CSTVwb72thaO98y3Jikxi01tpI5GLJoBciiXTvUxk+1ZHixH/+QbTDtKk5vla1vr+f183v/56xxZf3Vyuh8UuPph/oaz4KLpWjbG1U1vdU4GWyfCGyPBuNcOoQmmR1qYR0fNll0Y7Koe2RRf03WBnn97vVll3FwqtJqepnkzVuSydo+mpOTpA0GGR+bXpFlME7iPbp8s3WcSjZOVbZO7u+w08laMhmOZodYpLcbXF0ni27yUrKom2TRrh1u42ecrL/al3/ES2d7n7xSOfadShvu459ztiE63GyXPoD35Z/qyLpyBbTB/gJl+yWt2+WTRTd/OVnUNbKor7aOJ99+d/+uqNcG0xNrtv1cTa7NmpzjwRuP6ZBKJsOWNtrfiqlJUluV2u5kW89LIYtu9iKyqNNk0dx23Dqoql9M8uNJzrbW3jObd2eSf5Hk/iR/nOSvtda+s+Nrtemucv3Tsv7asmtg8fr43j06ZNE8ZNFq6ON792g5qDxqo+TiW8aLLRboLVkEzGuer4V/Kcn/nuSfXDbvkSRPtdY+XlWPzB7/Tzu9UI1b1l8TKsCe/FJkEdANv5QDyKPhsa286c/86aJqBA7J15a36F+KLAJmbpZFOzZ+Wmv/V1Xdf9Xsh5L88Oz+Y0k+nzk2tgYb4xx/7tWdngZwDVkEdMVB5dE7jn8nP/dn/+VBlwccsgeXtFxZBFzuZlm01xNB3NNaeyFJWmsvVNXdc/3WZJI6d36PiwS4hiwCumLXeXTrIPnhEy6dBxwoWQRcYx8XPZtPVT1cVU9X1dMbYxtawHLIIqALLs+il152yCmwHLIIVsteGz8vVtW9STKbnr3RE1trj7bWTrfWTq8PT+xxcQDXJYuArpgrjy7Porve3L+r6ABLJ4uAa+y18fNEkjOz+2eSPH4w5QDsiiwCukIeAV0gi4Br7Nj4qapPJvn1JO+squeq6mNJPp7kQ1X1bJIPzR4DLIwsArpCHgFdIIuAec1zVa+fvMGPlnUCe2AFySKgK+QR0AWyCJjXXq/qtUeVVB3uIgGuIYuA5Wtp2WxOqgoslyyC/jvcxs9wkMntpw51kQDXkEVAB7w8Xssvv3bvsssA9u2ryy5gX2QR9MWNs+hQGz9tWNl60/HDXCTANWQR0AXf3jyVT37zB5ddBrBv/++yC9gXWQR9ceMsOtzGT1XGa3u9kBjAwZBFQBdsjof51mu3LrsMYMXJIui/Q238TNYq5+9eO8xFAlxDFgFd0F4fZvO37lh2GcCKk0XQf4e7x88oOf8W37IDyyWLgC5YO9dyz29sLLsMYJ++vOwC9kkWQT/cLIsOt/GTpNnWApZMFgFdUJOW0QVX0gGWSxZB/9n0AQAAAOgpjR8AAACAntL4AQAAAOgpjR8AAACAntL4AQAAAOgpjR8AAACAntL4AQAAAOgpjR8AAACAnhotY6GDrWR4oaUmyWSUjI9V2nYLqpZREbCKZBEAANB3S2n8jM613PL8Zta+u5WLd67n3FuHGa9PN7jacBkVAatIFgEAAH23lEO9hpstx16+kNHZV7P+6mYGm0laUm06BTgMsggAAOi7pTR+2qAyOTZKO7Ge8drgjUMrWsXhFcChkUUAAEDfLeVQr8ko2bx1LW1Q2To1nB5SYSMLOGSyCAAA6Lsd9/ipqndU1b+rqmeq6ktV9dOz+XdW1ZNV9exsese8C22DZLJWGR8bZDKqS9+u2+ACbkAWAV2wiCwC2C1ZBOzGPId6bSX52621H0jygSR/o6reneSRJE+11h5I8tTs8Vwmo2TjlkE2bh9l60SlDWeHVgDcmCwCuuDAswhgD2QRMLcdGz+ttRdaa789u/9akmeSvD3JQ0kemz3tsSQfnnehk/XKhTsGef3uQS7ePshkbXYVnaWccQg4CmQR0AWLyCKA3ZJFwG7s6hw/VXV/kvcn+UKSe1prLyTT4Kmqu2/wOw8neThJ1m6d7mnYBklbTxxPAeyFLAK6YL9ZdOzY7YdUKdBnsgjYydzfa1fVLUn+VZKfaa29Ou/vtdYeba2dbq2dHp44tZcaAd4gi4AuOIgsWl+TRcD+yCJgHnM1fqpqLdNA+eXW2mdms1+sqntnP783ydnFlAgwJYuALpBFQBfIImBe81zVq5L8QpJnWms/d9mPnkhyZnb/TJLHD748gClZBHSBLAK6QBYBuzHPOX4+mOS/SfIfq+p3ZvP+bpKPJ/lUVX0sydeTfGQhFQJMySKgC2QR0AWyCJjbjo2f1tr/kxuf+fTBgy0H4PpkEdAFsgjoAlkE7IaLFgMAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE9p/AAAAAD0lMYPAAAAQE/t2PipquNV9RtV9e+r6ktV9fdn8++sqier6tnZ9I7FlwusKlkEdIEsArpAFgG7Mc8ePxeT/Ehr7b1J3pfkx6rqA0keSfJUa+2BJE/NHgMsiiwCukAWAV0gi4C57dj4aVPfnT1cm91akoeSPDab/1iSDy+iQIBEFgHdIIuALpBFwG7MdY6fqhpW1e8kOZvkydbaF5Lc01p7IUlm07tv8LsPV9XTVfX0+Py5AyobWEWyCOiCg8qijU1ZBOydLALmNVfjp7U2bq29L8l9SX6wqt4z7wJaa4+21k631k4PT5zaY5kAsgjohoPKovU1WQTsnSwC5rWrq3q11v40yeeT/FiSF6vq3iSZTc8edHEA1yOLgC6QRUAXyCJgJ/Nc1euuqnrT7P6JJD+a5PeTPJHkzOxpZ5I8vqAaAWQR0AmyCOgCWQTsxmiO59yb5LGqGmbaKPpUa+1zVfXrST5VVR9L8vUkH1lgnQCyCOgCWQR0gSwC5rZj46e19h+SvP86819O8uAiigK4miwCukAWAV0gi4Dd2NU5fgAAAAA4OjR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgp0bLLgDYpUpaTadJ0i5v37ak2pVTgIWQRUAHtKpMRpU2rLTBbDpMapLUuCUtGYxbBpuT1EQYAYvR9SzS+IEjqA2nG1ltML2fynQDazz9+WCcZGu2wQWwILIIWLY2rGydGma8Pt3o2jxVmawlg61keGG6oTW60LL+StP4ARam61mk8QNHzPY37G0wm842vGoy/XlNZo+XWSTQe7II6Io2rEzWKuP1ytaJymQ9GWwmaS1tqzLYmj4nW8uuFOizLmeRxg8cQZdvaE1Gbbob4TgZpN74xt3WFrBosghYtlbJZDTd0BqvJ+MTydbxZLiR1KTSNloGm7NmNcCCdD2LNH7gqKntYJl9w76WTIZJDZKkZTCuZLLsIoHek0VAB7ThdCNr63gyPlHZuK1lfKJlfHEaUsNRTb9xL50fYHG6nkVzX9WrqoZV9cWq+tzs8Z1V9WRVPTub3rG4MoErbJ9Qtdq0azxo0//N1f9vtGQRdIgskkXQAa2mJ1KdDJM2SibrLW27Kb1962kmySLoji5n0W4u5/7TSZ657PEjSZ5qrT2Q5KnZY4BFk0VAF8gioAtkEbCjuRo/VXVfkv8yyc9fNvuhJI/N7j+W5MMHWhnAVWQR0AWyCOgCWQTMa949fj6R5O/kyqP172mtvZAks+nd1/vFqnq4qp6uqqfH58/tp1aAT0QWAcv3iRxAFm1syiJgXz4RWQTMYcfGT1X9eJKzrbXf2ssCWmuPttZOt9ZOD0+c2stLAMgioBMOMovW12QRsDeyCNiNea7q9cEkP1FVfyXJ8SS3VdU/TfJiVd3bWnuhqu5NcnaRhQIrTxYBXSCLgC6QRcDcdtzjp7X2s621+1pr9yf5aJJfa639VJInkpyZPe1MkscXViWw8mQR0AWyCOgCWQTsxm6u6nW1jyf5UFU9m+RDs8cAh00WAV0gi4AukEXANeY51OsNrbXPJ/n87P7LSR48+JIAbk4WAV0gi4AukEXATvazxw8AAAAAHabxAwAAANBTGj8AAAAAPaXxA0dVu8EU4DDJIqALWlJtOs2kksns/vYN4DB0NIt2dXJnoCO2A2VSqdaSrUpNMr1t/wxg0WQRsGTVksG4ZbCVtI3K8EKlVTLcSAabs9uWPAIWq+tZpPEDR0278n5NktRs2sq3WsDhkEVAR9QkqfH0NthMBqNksFHTjazxdi4JJWCxupxFGj9wBFVLMs50Y2urktYuBc3luxMCLJAsApZu0jLYahluVJKW0flKWmW4kQwvtAw2k+Fms8cPsFgdzyKNHzhiqk07yG12hq7BZpKqK48ndYgFsGCyCOiCwbhl7buTjM5P0gaV8SuVybBSkzbdyBontdUy2Jwsu1Sgx7qeRRo/cNS0S7sQAiyNLAI6oCYto/Nbyy4DWHFdzyJX9QIAAADoKY0fAAAAgJ7S+AEAAADoKY0fAAAAgJ7S+AEAAADoKY0fAAAAgJ7S+AEAAADoKY0fAAAAgJ7S+AEAAADoKY0fAAAAgJ7S+AEAAADoqdE8T6qqP07yWpJxkq3W2umqujPJv0hyf5I/TvLXWmvfWUyZALII6AZZBHSBLALmtZs9fv6L1tr7WmunZ48fSfJUa+2BJE/NHgMsmiwCukAWAV0gi4Ad7edQr4eSPDa7/1iSD++7GoDdk0VAF8gioAtkEXCNeRs/LcmvVtVvVdXDs3n3tNZeSJLZ9O7r/WJVPVxVT1fV0+Pz5/ZfMbDKZBHQBQeSRRubsgjYF1kEzGWuc/wk+WBr7fmqujvJk1X1+/MuoLX2aJJHk+TEPe9oe6gRYJssArrgQLLotlvfLouA/ZBFwFzm2uOntfb8bHo2yWeT/GCSF6vq3iSZTc8uqkiARBYB3SCLgC6QRcC8dmz8VNWpqrp1+36Sv5Tkd5M8keTM7Glnkjy+qCIBZBHQBbII6AJZBOzGPId63ZPks1W1/fx/1lr7lar6zSSfqqqPJfl6ko8srkwAWQR0giwCukAWAXPbsfHTWvtKkvdeZ/7LSR5cRFEAV5NFQBfIIqALZBGwG/u5nDsAAAAAHabxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPaXxAwAAANBTGj8AAAAAPTVX46eq3lRVn66q36+qZ6rqh6rqzqp6sqqenU3vWHSxwGqTRUAXyCKgC2QRMK959/j5h0l+pbX2riTvTfJMkkeSPNVaeyDJU7PHAIski4AukEVAF8giYC47Nn6q6rYk/1mSX0iS1tpGa+1PkzyU5LHZ0x5L8uHFlAggi4BukEVAF8giYDfm2ePn+5K8lOQfV9UXq+rnq+pUkntaay8kyWx69wLrBJBFQBfIIqALZBEwt3kaP6Mkfy7JP2qtvT/Juexil8Gqeriqnq6qp8fnz+2xTABZBHTCgWXRxqYsAvZMFgFzm6fx81yS51prX5g9/nSmIfNiVd2bJLPp2ev9cmvt0dba6dba6eGJUwdRM7CaZBHQBQeWRetrsgjYM1kEzG3Hxk9r7VtJvlFV75zNejDJ7yV5IsmZ2bwzSR5fSIUAkUVAN8gioAtkEbAbozmf998n+eWqWk/ylSR/PdOm0aeq6mNJvp7kI4spEeANsgjoAlkEdIEsAuYyV+OntfY7SU5f50cPHmg1ADchi4AukEVAF8giYF7znOMHAAAAgCNI4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgpzR+AAAAAHpK4wcAAACgp3Zs/FTVO6vqdy67vVpVP1NVd1bVk1X17Gx6x2EUDKwmWQR0gSwCukAWAbuxY+Ontfbl1tr7WmvvS/KfJnk9yWeTPJLkqdbaA0memj0GWAhZBHSBLAK6QBYBu7HbQ70eTPJHrbWvJXkoyWOz+Y8l+fAB1gVwM7II6AJZBHSBLAJuareNn48m+eTs/j2ttReSZDa9+yALA7gJWQR0gSwCukAWATc1d+OnqtaT/ESSf7mbBVTVw1X1dFU9PT5/brf1AVxBFgFdcBBZtLEpi4D9kUXAPHazx89fTvLbrbUXZ49frKp7k2Q2PXu9X2qtPdpaO91aOz08cWp/1QLIIqAb9p1F62uyCNg3WQTsaDeNn5/MpV0Ik+SJJGdm988kefygigK4CVkEdIEsArpAFgE7mqvxU1Unk3woyWcum/3xJB+qqmdnP/v4wZcHcIksArpAFgFdIIuAeY3meVJr7fUkb75q3suZnkEe4FDIIqALZBHQBbIImNdur+oFAAAAwBGh8QMAAADQUxo/AAAAAD2l8QMAAADQUxo/AAAAAD2l8QMAAADQUxo/AAAAAD2l8QMAAADQUxo/AAAAAD2l8QMAAADQUxo/AAAAAD2l8QMAAADQU6NlFwAAsJKq0qqWXQWw6mQR9N6hNn5qkqyda4e5SIBryCKgC8brlVfvP7bsMoAVJ4ug/w618TMYJ+uv2tgClksWAV2wdTz5zruWXQWw6mQR9N/hHurVkuGmjS1gyWQR0AXDlq3bxsuuAlh1sgh673D3+Nma5PhLG4e5SIBryCKgC06c2Mj73/PVZZcB7NPXl13APski6IebZdHhnuNnc5y1F/70MBcJcA1ZBHTB96x/J//b/Z9ddhnAPj2+7AL2SRZBP9wsiw75UK+W2tw61EUCXEMWAR2wXsPcN7pl2WUAK04WQf8Nll0AAAAAAIsxV+Onqv5WVX2pqn63qj5ZVcer6s6qerKqnp1N71h0scBqk0VAF8gioAtkETCvHRs/VfX2JH8zyenW2nuSDJN8NMkjSZ5qrT2Q5KnZY4CFkEVAF8gioAtkEbAb8x7qNUpyoqpGSU4meT7JQ0kem/38sSQfPvDqAK4ki4AukEVAF8giYC47Nn5aa99M8g8yvTrYC0leaa39apJ7WmsvzJ7zQpK7F1kosNpkEdAFsgjoAlkE7MY8h3rdkWnn+HuTvC3Jqar6qXkXUFUPV9XTVfX0xvj83isFVposArrgILPopZfHiyoT6DlZBOzGPId6/WiSr7bWXmqtbSb5TJK/kOTFqro3SWbTs9f75dbao62106210+vDEwdVN7B6ZBHQBQeWRXe9eXhoRQO9I4uAuc3T+Pl6kg9U1cmqqiQPJnkmyRNJzsyecybJ44spESCJLAK6QRYBXSCLgLmNdnpCa+0LVfXpJL+dZCvJF5M8muSWJJ+qqo9lGjwfWWShwGqTRUAXyCKgC2QRsBs7Nn6SpLX295L8vatmX8y0swxwKGQR0AWyCOgCWQTMa97LuQMAAABwxGj8AAAAAPSUxg8AAABAT2n8AAAAAPSUxg8AAABAT1Vr7fAWVvVSknNJ/uTQFro8b4lx9skqjHM3Y/wzrbW7FlnMIs2y6GuxXvvEOPtl3nHKoqNjFcaYGGffyKJ+WoVxrsIYE+O82g2z6FAbP0lSVU+31k4f6kKXwDj7ZRXGuQpjvNoqjHkVxpgYZ9+syji3rcJ4V2GMiXH2zaqMc9uqjHcVxrkKY0yMczcc6gUAAADQUxo/AAAAAD21jMbPo0tY5jIYZ7+swjhXYYxXW4Uxr8IYE+Psm1UZ57ZVGO8qjDExzr5ZlXFuW5XxrsI4V2GMiXHO7dDP8QMAAADA4XCoFwAAAEBPHWrjp6p+rKq+XFV/WFWPHOayF6mq3lFV/66qnqmqL1XVT8/m31lVT1bVs7PpHcuudb+qalhVX6yqz80e93GMb6qqT1fV78/W6Q/1dJx/a/Z+/d2q+mRVHe/jOK9HFh399SqLejVOWSSLjixZ1KtxyiJZdGTJol6NcyFZdGiNn6oaJvk/kvzlJO9O8pNV9e7DWv6CbSX52621H0jygSR/Yza2R5I81Vp7IMlTs8dH3U8neeayx30c4z9M8iuttXcleW+m4+3VOKvq7Un+ZpLTrbX3JBkm+Wh6Ns7rkUW9Wa+yqAfjlEWyaIk1HhRZ1INxyiJZtMQaD4os6sE4F5pFrbVDuSX5oST/9rLHP5vkZw9r+Yd5S/J4kg8l+XKSe2fz7k3y5WXXts9x3Td7o/1Iks/N5vVtjLcl+Wpm57+6bH7fxvn2JN9IcmeSUZLPJflLfRvnDcYui474epVFvRqnLLr0WBYdsZss6tU4ZdGlx7LoiN1kUa/GubAsOsxDvbYHse252bxeqar7k7w/yReS3NNaeyFJZtO7l1jaQfhEkr+TZHLZvL6N8fuSvJTkH892l/z5qjqVno2ztfbNJP8gydeTvJDkldbar6Zn47wBWXT01+snIot6MU5ZJIuWWNpB+ERkUS/GKYtk0RJLOwifiCzqxTgXmUWH2fip68zr1SXFquqWJP8qyc+01l5ddj0Hqap+PMnZ1tpvLbuWBRsl+XNJ/lFr7f1JzuWI7zJ4PbPjQh9K8r1J3pbkVFX91HKrOjSy6AiTRf0ii64hi44IWdQvsugasuiIkEX9ssgsOszGz3NJ3nHZ4/uSPH+Iy1+oqlrLNFB+ubX2mdnsF6vq3tnP701ydln1HYAPJvmJqvrjJP88yY9U1T9Nv8aYTN+nz7XWvjB7/OlMQ6Zv4/zRJF9trb3UWttM8pkkfyH9G+f1yKKjvV5lUb/GKYsukUVHiyzq1zhl0SWy6GiRRf0a58Ky6DAbP7+Z5IGq+t6qWs/0JEVPHOLyF6aqKskvJHmmtfZzl/3oiSRnZvfPZHpc6ZHUWvvZ1tp9rbX7M113v9Za+6n0aIxJ0lr7VpJvVNU7Z7MeTPJ76dk4M9198ANVdXL2/n0w0xOk9W2c1yOLjvB6lUX9GmdkkSw6omRRv8YZWSSLjihZ1K9xZoFZVLMTBB2KqvormR6DOEzyi621/+XQFr5AVfUXk/zfSf5jLh1b+XczPYb0U0m+J9OV+JHW2reXUuQBqqofTvI/ttZ+vKrenJ6Nsarel+Tnk6wn+UqSv55pk7Rv4/z7Sf6rTK948MUk/22SW9KzcV6PLOrHepVFvRmnLJJFR5os6s04ZZEsOtJkUW/GuZAsOtTGDwAAAACH5zAP9QIAAADgEGn8AAAAAPSUxg8AAABAT2n8AAAAAPSUxg8AAABAT2n8AAAAAPSUxg8AAABAT2n8AAAAAPTU/w9LP1zrJ8ahyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_env('PongNoFrameskip-v4')\n",
    "obs = env.reset()\n",
    "\n",
    "s,r,d,i = env.step(0) # Run first step\n",
    "s,r,d,i = env.step(0) # Run second step\n",
    "print(s.shape, r, d, i)\n",
    "fig,ax = plt.subplots(1, 4, figsize = (20, 8))\n",
    "for ind in range(4):\n",
    "    obs_temp = s[ind, :]\n",
    "    ax[ind].imshow(obs_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Class: DeepQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQNetwork(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model = DeepQNetwork(lr = 0.0001, n_actions = env.action_space.n, input_dims = env.observation_space.shape,\\\n",
    "                          name = '_q_eval', chkpt_dir = 'Models/')\n",
    "temp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Class: ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "(10, 1)\n",
      "(10, 1)\n",
      "[0 0 0 0 0 0 0 0 0 0] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [False False False False False False False False False False]\n",
      "\n",
      "7\n",
      "\n",
      "[0.] [3.] 1 2.0 False\n",
      "[1.] [4.] 2 3.0 True\n",
      "[2.] [5.] 3 4.0 False\n",
      "[3.] [6.] 4 5.0 True\n",
      "[4.] [7.] 5 6.0 False\n",
      "[5.] [8.] 6 7.0 True\n",
      "[6.] [9.] 7 8.0 False\n"
     ]
    }
   ],
   "source": [
    "rp_buff = ReplayBuffer(10, [1])\n",
    "print(rp_buff.mem_size)\n",
    "print(rp_buff.mem_cntr)\n",
    "print(rp_buff.state_memory.shape)\n",
    "print(rp_buff.new_state_memory.shape)\n",
    "print(rp_buff.action_memory, rp_buff.reward_memory, rp_buff.terminal_memory)\n",
    "print()\n",
    "\n",
    "for ind in range(7):\n",
    "    rp_buff.store_transition([ind], ind+1, ind+2, [ind + 3], ind % 2)\n",
    "\n",
    "print(rp_buff.mem_cntr)\n",
    "print()\n",
    "for ind in range(rp_buff.mem_cntr):\n",
    "    print(rp_buff.state_memory[ind], rp_buff.new_state_memory[ind], rp_buff.action_memory[ind],\\\n",
    "         rp_buff.reward_memory[ind], rp_buff.terminal_memory[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [5.],\n",
       "        [2.]], dtype=float32),\n",
       " array([7, 2, 4, 6, 3], dtype=int64),\n",
       " array([8., 3., 5., 7., 4.], dtype=float32),\n",
       " array([[9.],\n",
       "        [4.],\n",
       "        [6.],\n",
       "        [8.],\n",
       "        [5.]], dtype=float32),\n",
       " array([False,  True,  True,  True, False]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_buff.sample_buffer(batch_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score:  -18.0  average score -18.0 best score -inf epsilon 0.99 steps 1107\n",
      "episode:  1 score:  -20.0  average score -19.0 best score -18.00 epsilon 0.98 steps 2005\n",
      "episode:  2 score:  -21.0  average score -19.7 best score -18.00 epsilon 0.97 steps 2887\n",
      "episode:  3 score:  -21.0  average score -20.0 best score -18.00 epsilon 0.96 steps 3708\n",
      "episode:  4 score:  -19.0  average score -19.8 best score -18.00 epsilon 0.95 steps 4647\n",
      "episode:  5 score:  -21.0  average score -20.0 best score -18.00 epsilon 0.95 steps 5472\n",
      "episode:  6 score:  -21.0  average score -20.1 best score -18.00 epsilon 0.94 steps 6283\n",
      "episode:  7 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.93 steps 7048\n",
      "episode:  8 score:  -20.0  average score -20.2 best score -18.00 epsilon 0.92 steps 8296\n",
      "episode:  9 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.91 steps 9257\n"
     ]
    }
   ],
   "source": [
    "env = make_env('PongNoFrameskip-v4')\n",
    "obs = env.reset()\n",
    "best_score = -np.inf # Reset best score\n",
    "load_checkpoint = False\n",
    "n_games = 10 # Number of games (episodes) to play \n",
    "n_steps = 0 # Number of time steps run till now\n",
    "scores = [] # Placeholder to store cumulative rewards for each episode (no discounting)\n",
    "steps_array = [] # Placeholder to store number of steps executed in each episode\n",
    "eps_history = [] # Placeholder to store epsilon value of agent at the end of each episode\n",
    "\n",
    "# Instantiate DQN agent\n",
    "agent = DQNAgent(gamma = 0.99, epsilon = 1, lr = 1e-4, input_dims = (env.observation_space.shape),\\\n",
    "                 n_actions = env.action_space.n, mem_size = 50000, eps_min = 0.1, batch_size = 32,\\\n",
    "                 replace = 1000, eps_dec = 1e-5, chkpt_dir = 'models/', algo = 'DQNAgent',\\\n",
    "                 env_name = 'PongNoFrameskip-v4')\n",
    "\n",
    "# Load saved model if load_checkpoint is true.\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "for i in range(n_games): # Iterate through n_games (episodes)\n",
    "    done = False # Reset 'done' flag\n",
    "    observation = env.reset() # Reset environment at the start of the episode\n",
    "    score = 0 # Initialize score to 0.\n",
    "    while not done: # Loop till the end of the game (episode)\n",
    "        action = agent.choose_action(observation) # Choose action based on eps-greedy policy\n",
    "        observation_, reward, done, info = env.step(action) # Perform one step\n",
    "        score += reward # Increment score of current episode\n",
    "        if not load_checkpoint: # Update replay memory and perform one learning step\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "        observation = observation_ # Replace current observation with next observation\n",
    "        n_steps += 1 # Update n_steps\n",
    "        #print(n_steps, end = \", \")\n",
    "    scores.append(score) # Update scores array at the end of current game\n",
    "    steps_array.append(n_steps) # Update steps array at the end of current game\n",
    "    eps_history.append(agent.epsilon) # Update eps_history\n",
    "    avg_score = np.mean(scores[-100:]) \n",
    "    print('episode: ', i,'score: ', score, ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\\\n",
    "          'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
    "    if avg_score > best_score: # Update best_score if current episode score better than previous best_score\n",
    "        if not load_checkpoint:\n",
    "            #agent.save_models()\n",
    "            best_score = avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1107, 2005, 2887, 3708, 4647, 5472, 6283, 7048, 8296, 9257]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from dqn_agent import DQNAgent\n",
    "from utils import plot_learning_curve, make_env\n",
    "from gym import wrappers\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = make_env('PongNoFrameskip-v4')\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    best_score = -np.inf\n",
    "    load_checkpoint = False\n",
    "    n_games = 250\n",
    "\n",
    "    agent = DQNAgent(gamma=0.99, epsilon=1, lr=0.0001,\n",
    "                     input_dims=(env.observation_space.shape),\n",
    "                     n_actions=env.action_space.n, mem_size=50000, eps_min=0.1,\n",
    "                     batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                     chkpt_dir='models/', algo='DQNAgent',\n",
    "                     env_name='PongNoFrameskip-v4')\n",
    "\n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "\n",
    "    fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_' \\\n",
    "            + str(n_games) + 'games'\n",
    "    figure_file = 'plots/' + fname + '.png'\n",
    "    # if you want to record video of your agent playing, do a mkdir tmp && mkdir tmp/dqn-video\n",
    "    # and uncomment the following 2 lines.\n",
    "    #env = wrappers.Monitor(env, \"tmp/dqn-video\",\n",
    "    #                    video_callable=lambda episode_id: True, force=True)\n",
    "    n_steps = 0\n",
    "    scores, eps_history, steps_array = [], [], []\n",
    "\n",
    "    for i in range(n_games):\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            if not load_checkpoint:\n",
    "                agent.store_transition(observation, action,\n",
    "                                     reward, observation_, done)\n",
    "                agent.learn()\n",
    "            observation = observation_\n",
    "            n_steps += 1\n",
    "        scores.append(score)\n",
    "        steps_array.append(n_steps)\n",
    "\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print('episode: ', i,'score: ', score,\n",
    "             ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
    "            'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            if not load_checkpoint:\n",
    "                agent.save_models()\n",
    "            best_score = avg_score\n",
    "\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "    x = [i+1 for i in range(len(scores))]\n",
    "    plot_learning_curve(steps_array, scores, eps_history, figure_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
