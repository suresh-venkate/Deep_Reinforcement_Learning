{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# N-step Deep Q-Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJIVLLT1nYMl"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install \\\n",
        "  gym[atari,accept-rom-license]==0.23.1 \\\n",
        "  pytorch-lightning==1.6.0 \\\n",
        "  stable-baselines3 \\\n",
        "  pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import TransformObservation, NormalizeObservation, \\\n",
        "  NormalizeReward, RecordVideo, RecordEpisodeStatistics, AtariPreprocessing\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLH52SgC0RRI"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZO9u9X1CTtf"
      },
      "outputs": [],
      "source": [
        "import math \n",
        "from torch.nn.init import kaiming_uniform_, zeros_\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_features, sigma):\n",
        "    super(NoisyLinear, self).__init__()\n",
        "    self.w_mu = nn.Parameter(torch.empty((out_features, in_features)))\n",
        "    self.w_sigma = nn.Parameter(torch.empty((out_features, in_features)))\n",
        "    self.b_mu = nn.Parameter(torch.empty((out_features)))\n",
        "    self.b_sigma = nn.Parameter(torch.empty((out_features)))\n",
        "\n",
        "    kaiming_uniform_(self.w_mu, a=math.sqrt(5))\n",
        "    kaiming_uniform_(self.w_sigma, a=math.sqrt(5))\n",
        "    zeros_(self.b_mu)\n",
        "    zeros_(self.b_sigma)\n",
        "    \n",
        "  def forward(self, x, sigma=0.5):\n",
        "    if self.training:\n",
        "      w_noise = torch.normal(0, sigma, size=self.w_mu.size()).to(device)\n",
        "      b_noise = torch.normal(0, sigma, size=self.b_mu.size()).to(device)\n",
        "      return F.linear(x, self.w_mu + self.w_sigma * w_noise, self.b_mu + self.b_sigma * b_noise)\n",
        "    else:\n",
        "      return F.linear(x, self.W_mu, self.b_mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gm8-15nYq7"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_shape, n_actions, sigma=0.5):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(obs_shape[0], 64, kernel_size=3),\n",
        "      nn.MaxPool2d(kernel_size=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3),\n",
        "      nn.MaxPool2d(kernel_size=4),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "    conv_out_size = self._get_conv_out(obs_shape)\n",
        "    print(conv_out_size)\n",
        "    self.head = nn.Sequential(\n",
        "      NoisyLinear(conv_out_size, hidden_size, sigma=sigma),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc_adv = NoisyLinear(hidden_size, n_actions, sigma=sigma) \n",
        "    self.fc_value = NoisyLinear(hidden_size, 1, sigma=sigma)\n",
        "\n",
        "  def _get_conv_out(self, shape):\n",
        "    conv_out = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(conv_out.size()))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv(x.float()).view(x.size()[0], -1)\n",
        "    x = self.head(x)\n",
        "    adv = self.fc_adv(x)\n",
        "    value = self.fc_value(x)\n",
        "    return value + adv - torch.mean(adv, dim=1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "def greedy(state, net):\n",
        "  state = torch.tensor([state]).to(device)\n",
        "  q_values = net(state)\n",
        "  _, action = torch.max(q_values, dim=1)\n",
        "  action = int(action.item())\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brJmKGkl0jge"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvHMYqlZnYvj"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "    self.priorities = deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    self.alpha = 0.0  # anneal.\n",
        "    self.beta = 1.0  # anneal.\n",
        "    self.max_priority = 0.0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "    self.priorities.append(self.max_priority)\n",
        "  \n",
        "  def update(self, index, priority):\n",
        "    if priority > self.max_priority:\n",
        "      self.max_priority = priority\n",
        "    self.priorities[index] = priority\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    prios = np.array(self.priorities, dtype=np.float64) + 1e-4 # Stability constant.\n",
        "    prios = prios ** self.alpha\n",
        "    probs = prios / prios.sum()\n",
        "\n",
        "    weights = (self.__len__() * probs) ** -self.beta\n",
        "    weights = weights / weights.max()\n",
        "\n",
        "    idx = random.choices(range(self.__len__()), weights=probs, k=batch_size)\n",
        "    sample = [(i, weights[i], *self.buffer[i]) for i in idx]\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQcRQ4xnYyI"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvDC9qF0oPr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND0H6-U8FziY"
      },
      "outputs": [],
      "source": [
        "env = gym.make('PongNoFrameskip-v4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcI0wQKGH5MS"
      },
      "outputs": [],
      "source": [
        "env.observation_space, env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0s9N1xmH5RQ"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "i = 60\n",
        "skip = 1\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  frames.append(obs)\n",
        "  obs, _, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "frames = np.hstack([frames[i], frames[i+skip], frames[i+2*skip]])\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.axis('off')\n",
        "plt.imshow(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY008LYyIUWO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEIS5AcXIUYR"
      },
      "outputs": [],
      "source": [
        "env = AtariPreprocessing(env, frame_skip=8, screen_size=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjHlBPxxFzlM"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "i = 170\n",
        "skip = 1\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  frames.append(obs)\n",
        "  obs, _, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "img = np.hstack([frames[i], frames[i+skip], frames[i+2*skip]])\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.axis('off')\n",
        "plt.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaPOytEkH5EW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlLAfAwuL-YJ"
      },
      "outputs": [],
      "source": [
        "env = NormalizeObservation(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnLy0zfCL-aT"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "i = 120\n",
        "skip = 1\n",
        "\n",
        "for i in range(20):\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    frames.append(obs)\n",
        "    obs, _, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "img = np.hstack([frames[i], frames[i+skip], frames[i+2*skip]])\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.axis('off')\n",
        "plt.imshow(img.squeeze(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw-CWX4nap9c"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, 'videos', episode_trigger=lambda e: e % 100 == 0)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  env = AtariPreprocessing(env, frame_skip=8, screen_size=42)\n",
        "  env = TransformObservation(env, lambda x: x[np.newaxis,:,:])\n",
        "  env.observation_space = gym.spaces.Box(low=0, high=1, shape=(1, 42, 42), dtype=np.float32)\n",
        "  env = NormalizeObservation(env)\n",
        "  env = NormalizeReward(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2kQbrqV066I"
      },
      "outputs": [],
      "source": [
        "env = create_environment('PongNoFrameskip-v4')\n",
        "frames = []\n",
        "for episode in range(10):\n",
        "  done = False\n",
        "  obs = env.reset()\n",
        "  while not done:\n",
        "    frames.append(obs)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done, _ = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL7ydsCHSA8T"
      },
      "outputs": [],
      "source": [
        "display_video(episode=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "\n",
        "  # Initialize.\n",
        "  def __init__(self, env_name, policy=greedy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, samples_per_epoch=10_000, \n",
        "               sync_rate=10, sigma=0.5, a_start=0.5, a_end=0.0, a_last_episode=100, \n",
        "               b_start=0.4, b_end=1.0, b_last_episode=100, n_steps=3):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape\n",
        "    n_actions = self.env.action_space.n\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, n_actions, sigma=sigma)\n",
        "\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.policy = policy\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode()\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None):\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "    transitions = []\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(state, self.q_net)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "      \n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      transitions.append(exp)\n",
        "      state = next_state\n",
        "\n",
        "    for i, (s, a, r, d, ns) in enumerate(transitions):\n",
        "      batch = transitions[i:i+self.hparams.n_steps]\n",
        "      ret = sum([t[2] * self.hparams.gamma**j for j, t in enumerate(batch)])\n",
        "      _, _, _, ld, ls = batch[-1]\n",
        "      self.buffer.append((s, a, ret, ld, ls))\n",
        "\n",
        "  # Forward.\n",
        "  def forward(self, x):\n",
        "    return self.q_net(x)\n",
        "\n",
        "  # Configure optimizers.\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  # Create dataloader.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  # Training step.\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    indices, weights, states, actions, returns, dones, next_states = batch\n",
        "    weights = weights.unsqueeze(1)\n",
        "    actions = actions.unsqueeze(1)\n",
        "    returns = returns.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    state_action_values = self.q_net(states).gather(1, actions)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      _, next_actions = self.q_net(next_states).max(dim=1, keepdim=True)\n",
        "      next_action_values = self.target_q_net(next_states).gather(1, next_actions)\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "    expected_state_action_values = returns + self.hparams.gamma**self.hparams.n_steps * next_action_values\n",
        "\n",
        "    td_errors = (state_action_values - expected_state_action_values).abs().detach()\n",
        "\n",
        "    for idx, e in zip(indices, td_errors):\n",
        "      self.buffer.update(idx, e.item())\n",
        "\n",
        "    loss = weights * self.hparams.loss_fn(state_action_values, expected_state_action_values, reduction='none')\n",
        "    loss = loss.mean()\n",
        "\n",
        "    self.log('episode/Q-Error', loss)\n",
        "    return loss\n",
        "\n",
        "  # Training epoch end.\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    alpha = max(\n",
        "        self.hparams.a_end,\n",
        "        self.hparams.a_start - self.current_epoch / self.hparams.a_last_episode\n",
        "    )\n",
        "    beta = min(\n",
        "        self.hparams.b_end,\n",
        "        self.hparams.b_start + self.current_epoch / self.hparams.b_last_episode\n",
        "    )\n",
        "    self.buffer.alpha = alpha\n",
        "    self.buffer.beta = beta\n",
        "\n",
        "    self.play_episode(policy=self.policy)\n",
        "    self.log('episode/Return', self.env.return_queue[-1])\n",
        "\n",
        "    if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "      self.target_q_net.load_state_dict(self.q_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algo = DeepQLearning(\n",
        "  'PongNoFrameskip-v4',\n",
        "  lr=1e-4,\n",
        "  sigma=0.5,\n",
        "  hidden_size=256,\n",
        "  a_last_episode=8_000,\n",
        "  b_last_episode=8_000,\n",
        "  n_steps=8\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  gpus=num_gpus,\n",
        "  max_epochs=10_000,\n",
        "  log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD3x39w71xWR"
      },
      "source": [
        "#### Check the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PleQkLR-yNM"
      },
      "outputs": [],
      "source": [
        "display_video(episode=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRwyLvCdCOO3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1i81Gz-kIOT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "8_n_step_dqn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}