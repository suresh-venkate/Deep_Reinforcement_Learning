{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCxxWBZioi0N"
   },
   "source": [
    "# Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJIVLLT1nYMl"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb\n",
    "\n",
    "!pip install pygame gym==0.18 stable-baselines3 pytorch-lightning==1.6.0 pyvirtualdisplay\n",
    "\n",
    "!pip install git+https://github.com/GrupoTuring/PyGame-Learning-Environment\n",
    "!pip install git+https://github.com/lusob/gym-ple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOSJl-X7zvs4"
   },
   "source": [
    "#### Setup virtual display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-Z6takfzqGk"
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "Display(visible=False, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz8DLleGz_TF"
   },
   "source": [
    "#### Import the necessary code libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cP5t6U7-nYoc"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import gym\n",
    "import gym_ple\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv, WarpFrame\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_IrPlU1wwPx"
   },
   "outputs": [],
   "source": [
    "# Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb#scrollTo=gKc1FNhKiVJX\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLH52SgC0RRI"
   },
   "source": [
    "#### Create the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6gm8-15nYq7"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "  def __init__(self, hidden_size, obs_shape, n_actions):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(obs_shape[0], 64, kernel_size=3),\n",
    "        nn.MaxPool2d(kernel_size=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3),\n",
    "        nn.MaxPool2d(kernel_size=4),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    conv_out_size = self._get_conv_out(obs_shape)\n",
    "    self.head = nn.Sequential(\n",
    "        nn.Linear(conv_out_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc_adv = nn.Linear(hidden_size, n_actions)\n",
    "    self.fc_value = nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def _get_conv_out(self, shape):\n",
    "    conv_out = self.conv(torch.zeros(1, *shape))\n",
    "    return int(np.prod(conv_out.size()))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x.float()).view(x.size()[0], -1)\n",
    "    x = self.head(x)\n",
    "    adv = self.fc_adv(x)\n",
    "    value = self.fc_value(x)\n",
    "    return value + adv - torch.mean(adv, dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnk0wSWj0hAz"
   },
   "source": [
    "#### Create the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9a0b9cdnYtT"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
    "  if np.random.random() < epsilon:\n",
    "    action = env.action_space.sample()\n",
    "  else:\n",
    "    state = torch.tensor([state]).to(device)\n",
    "    q_values = net(state)\n",
    "    _, action = torch.max(q_values, dim=1)\n",
    "    action = int(action.item())\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brJmKGkl0jge"
   },
   "source": [
    "#### Create the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvHMYqlZnYvj"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "  # Constructor.\n",
    "  def __init__(self, capacity):\n",
    "    self.buffer = deque(maxlen=capacity)\n",
    "    self.priorities = deque(maxlen=capacity)\n",
    "    self.capacity = capacity\n",
    "    self.alpha = 1.0\n",
    "    self.beta = 0.5\n",
    "    self.max_priority = 0.0\n",
    "\n",
    "  # __len__\n",
    "  def __len__(self):\n",
    "    return len(self.buffer)\n",
    "\n",
    "  # Append.\n",
    "  def append(self, experience):\n",
    "    self.buffer.append(experience)\n",
    "    self.priorities.append(self.max_priority)\n",
    "\n",
    "  # Update.\n",
    "  def update(self, index, priority):\n",
    "    if priority > self.max_priority:\n",
    "      self.max_priority = priority\n",
    "    self.priorities[index] = priority\n",
    "\n",
    "  # Sample.\n",
    "  def sample(self, batch_size):\n",
    "    prios = np.array(self.priorities, dtype=np.float64) + 1e-4\n",
    "    prios = prios ** self.alpha\n",
    "    probs = prios / prios.sum()\n",
    "\n",
    "    weights = (self.__len__() * probs) ** -self.beta\n",
    "    weights = weights / weights.max()\n",
    "\n",
    "    idx = random.choices(range(self.__len__()), weights=probs, k=batch_size)\n",
    "    sample = [(i, weights[i], *self.buffer[i]) for i in idx]\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUQcRQ4xnYyI"
   },
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "\n",
    "  def __init__(self, buffer, sample_size=400):\n",
    "    self.buffer = buffer\n",
    "    self.sample_size = sample_size\n",
    "  \n",
    "  def __iter__(self):\n",
    "    for experience in self.buffer.sample(self.sample_size):\n",
    "      yield experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yvDC9qF0oPr"
   },
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCP9uEZCngXH"
   },
   "outputs": [],
   "source": [
    "class RunningMeanStd:\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, \"float64\")\n",
    "        self.var = np.ones(shape, \"float64\")\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n",
    "        )\n",
    "\n",
    "\n",
    "def update_mean_var_count_from_moments(\n",
    "    mean, var, count, batch_mean, batch_var, batch_count\n",
    "):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "\n",
    "\n",
    "class NormalizeObservation(gym.core.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        epsilon=1e-8,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n",
    "        if self.is_vector_env:\n",
    "            self.obs_rms = RunningMeanStd(shape=self.single_observation_space.shape)\n",
    "        else:\n",
    "            self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, rews, dones, infos = self.env.step(action)\n",
    "        if self.is_vector_env:\n",
    "            obs = self.normalize(obs)\n",
    "        else:\n",
    "            obs = self.normalize(np.array([obs]))[0]\n",
    "        return obs, rews, dones, infos\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return_info = kwargs.get(\"return_info\", False)\n",
    "        if return_info:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        if self.is_vector_env:\n",
    "            obs = self.normalize(obs)\n",
    "        else:\n",
    "            obs = self.normalize(np.array([obs]))[0]\n",
    "        if not return_info:\n",
    "            return obs\n",
    "        else:\n",
    "            return obs, info\n",
    "\n",
    "    def normalize(self, obs):\n",
    "        self.obs_rms.update(obs)\n",
    "        return (obs - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon)\n",
    "\n",
    "\n",
    "class NormalizeReward(gym.core.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        gamma=0.99,\n",
    "        epsilon=1e-8,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n",
    "        self.return_rms = RunningMeanStd(shape=())\n",
    "        self.returns = np.zeros(self.num_envs)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, rews, dones, infos = self.env.step(action)\n",
    "        if not self.is_vector_env:\n",
    "            rews = np.array([rews])\n",
    "        self.returns = self.returns * self.gamma + rews\n",
    "        rews = self.normalize(rews)\n",
    "        self.returns[dones] = 0.0\n",
    "        if not self.is_vector_env:\n",
    "            rews = rews[0]\n",
    "        return obs, rews, dones, infos\n",
    "\n",
    "    def normalize(self, rews):\n",
    "        self.return_rms.update(self.returns)\n",
    "        return rews / np.sqrt(self.return_rms.var + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r52lpjPFhFzd"
   },
   "outputs": [],
   "source": [
    "env = gym_ple.make(\"FlappyBird-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTdRGXMHhHdj"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V04aLOL2gjR9"
   },
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0ppG1ncTSws"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "  obs, rew, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6po4CHxHtyI7"
   },
   "outputs": [],
   "source": [
    "env = MaxAndSkipEnv(env, skip=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXMabRn9t4Mw"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(10):\n",
    "  obs, _, _, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7rCRq-qtyLN"
   },
   "outputs": [],
   "source": [
    "type(obs), obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4TQZfU9tyNw"
   },
   "outputs": [],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMGcAo7USiZK"
   },
   "outputs": [],
   "source": [
    "env = WarpFrame(env, height=42, width=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRKvoPFZdrsP"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(10):\n",
    "  obs, _, _, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L1nEL29druP"
   },
   "outputs": [],
   "source": [
    "type(obs), obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3n-dMridr41"
   },
   "outputs": [],
   "source": [
    "plt.imshow(obs.squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgnxpk1oey11"
   },
   "outputs": [],
   "source": [
    "obs.min(), obs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8E4MD8nv0TF"
   },
   "outputs": [],
   "source": [
    "env = TransformObservation(env, lambda x: x.swapaxes(-1, 0)) \n",
    "env.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, 42, 42), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3mWvT2Dv0Vk"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(10):\n",
    "  obs, _, _, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eg74mT4khdlY"
   },
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1t7sCsXhbHd"
   },
   "outputs": [],
   "source": [
    "def create_environment(env_name):\n",
    "  env = gym_ple.make(env_name)\n",
    "  env = MaxAndSkipEnv(env, skip=2)\n",
    "  env = WarpFrame(env, height=42, width=42)\n",
    "  env = TransformObservation(env, lambda x: x.swapaxes(-1, 0))\n",
    "  env.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, 42, 42), dtype=np.float32)\n",
    "  env = NormalizeObservation(env)\n",
    "  env = NormalizeReward(env)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgXi6A4Z1p75"
   },
   "source": [
    "#### Create the Deep Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3p09qCyHhaoW"
   },
   "outputs": [],
   "source": [
    "class DeepQLearning(LightningModule):\n",
    "\n",
    "  # Initialize.\n",
    "  def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000, \n",
    "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
    "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15, \n",
    "               eps_last_episode=100, samples_per_epoch=1_000, sync_rate=10,\n",
    "               a_start=0.5, a_end=0.0, a_last_episode=100,\n",
    "               b_start=0.4, b_end=1.0, b_last_episode=100):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.env = create_environment(env_name)\n",
    "\n",
    "    obs_size = self.env.observation_space.shape\n",
    "    n_actions = self.env.action_space.n\n",
    "\n",
    "    self.q_net = DQN(hidden_size, obs_size, n_actions)\n",
    "\n",
    "    self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "    self.policy = policy\n",
    "    self.buffer = ReplayBuffer(capacity=capacity)\n",
    "\n",
    "    self.save_hyperparameters()\n",
    "\n",
    "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
    "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
    "      self.play_episode(epsilon=self.hparams.eps_start)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def play_episode(self, policy=None, epsilon=0.):\n",
    "    state = self.env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      if policy:\n",
    "        action = policy(state, self.env, self.q_net, epsilon=epsilon)\n",
    "      else:\n",
    "        action = self.env.action_space.sample()\n",
    "      next_state, reward, done, info = self.env.step(action)\n",
    "      exp = (state, action, reward, done, next_state)\n",
    "      self.buffer.append(exp)\n",
    "      state = next_state\n",
    "\n",
    "  # Forward.\n",
    "  def forward(self, x):\n",
    "    return self.q_net(x)\n",
    "\n",
    "  # Configure optimizers.\n",
    "  def configure_optimizers(self):\n",
    "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "    return [q_net_optimizer]\n",
    "\n",
    "  # Create dataloader.\n",
    "  def train_dataloader(self):\n",
    "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=self.hparams.batch_size\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "  # Training step.\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    indices, weights, states, actions, rewards, dones, next_states = batch\n",
    "    weights = weights.unsqueeze(1)\n",
    "    actions = actions.unsqueeze(1)\n",
    "    rewards = rewards.unsqueeze(1)\n",
    "    dones = dones.unsqueeze(1)\n",
    "\n",
    "    state_action_values = self.q_net(states).gather(1, actions)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      _, next_actions = self.q_net(next_states).max(dim=1, keepdim=True)\n",
    "      next_action_values = self.target_q_net(next_states).gather(1, next_actions)\n",
    "      next_action_values[dones] = 0.0\n",
    "\n",
    "    expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
    "\n",
    "    td_errors = (state_action_values - expected_state_action_values).abs().detach()\n",
    "\n",
    "    for idx, e in zip(indices, td_errors):\n",
    "      self.buffer.update(idx, e.cpu().item())\n",
    "\n",
    "    loss = weights * self.hparams.loss_fn(state_action_values, expected_state_action_values, reduction='none')\n",
    "    loss = loss.mean()\n",
    "\n",
    "    self.log('episode/Q-Error', loss)\n",
    "    return loss\n",
    "\n",
    "  def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "    epsilon = max(\n",
    "        self.hparams.eps_end,\n",
    "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
    "    )\n",
    "    alpha = max(\n",
    "        self.hparams.a_end,\n",
    "        self.hparams.a_start - self.current_epoch / self.hparams.a_last_episode\n",
    "    )\n",
    "    beta = max(\n",
    "        self.hparams.b_end,\n",
    "        self.hparams.b_start - self.current_epoch / self.hparams.b_last_episode\n",
    "    )\n",
    "    self.buffer.alpha = alpha\n",
    "    self.buffer.beta = beta\n",
    "\n",
    "    self.play_episode(policy=self.policy, epsilon=epsilon)\n",
    "    self.log('episode/Return', self.env.unwrapped.game_state.score())\n",
    "\n",
    "    if self.current_epoch % self.hparams.sync_rate == 0:\n",
    "      self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mm9P0sX1wAA"
   },
   "source": [
    "#### Purge logs and run the visualization tool (Tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfGQdpn0nY99"
   },
   "outputs": [],
   "source": [
    "!rm -r /content/lightning_logs/\n",
    "!rm -r /content/videos/\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8GdIwla1wrW"
   },
   "source": [
    "#### Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig8c_RM8nZLN"
   },
   "outputs": [],
   "source": [
    "algo = DeepQLearning(\n",
    "    'Snake-v0',\n",
    "    lr=5e-4,\n",
    "    hidden_size=512,\n",
    "    eps_end=0.01,\n",
    "    eps_last_episode=1_000,\n",
    "    capacity=10_000,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=num_gpus,\n",
    "    max_epochs=3_000,\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "\n",
    "trainer.fit(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD3x39w71xWR"
   },
   "source": [
    "#### Check the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpqIZa5hxPmn"
   },
   "outputs": [],
   "source": [
    "env = algo.env\n",
    "policy = algo.policy\n",
    "q_net = algo.q_net.cuda()\n",
    "frames = []\n",
    "\n",
    "for episode in range(10):\n",
    "  done = False\n",
    "  obs = env.reset()\n",
    "  while not done:\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = policy(obs, env, q_net)\n",
    "    obs, _, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OryDNmZXxQ0C"
   },
   "outputs": [],
   "source": [
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_so21fxx0md"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "6_prioritized_experience_replay.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
